# Dify 模型配置指南

Dify 支持多种 LLM（大型语言模型）的配置，包括商业模型和开源模型。本指南将帮助您了解如何在 Dify 中配置和使用各种模型。

## 目录

- [支持的模型提供商](#支持的模型提供商)
- [配置模型](#配置模型)
- [模型管理](#模型管理)
- [模型性能比较](#模型性能比较)
- [自托管模型配置](#自托管模型配置)

## 支持的模型提供商

Dify 支持以下模型提供商：

### 商业模型

- **OpenAI** - GPT-3.5-Turbo, GPT-4, GPT-4o 等
- **Anthropic** - Claude, Claude 2, Claude 3 等
- **Google** - Gemini, PaLM 等
- **Azure OpenAI**
- **阿里云通义千问**
- **百度文心一言**
- **讯飞星火**
- **智谱 ChatGLM**
- **MiniMax**
- **AWS Bedrock**
- **Cohere**
- **更多...**

### 开源模型

- **Mistral AI**
- **Meta Llama 2, Llama 3**
- **Yi 易模型**
- **Qwen 通义千问开源版**
- **Baichuan**
- **DeepSeek**
- **更多...**

## 配置模型

### 添加模型提供商

1. 在 Dify 控制台中，导航至**设置 > 模型提供商**
2. 点击**添加模型提供商**按钮
3. 选择您要添加的提供商
4. 输入相应的 API 密钥和设置
5. 点击**保存**

### 配置自定义模型

如果您需要添加自定义模型或自托管模型：

1. 选择**自定义模型**选项
2. 输入模型 API 端点
3. 配置相关参数（如温度、最大令牌数等）
4. 测试连接以确保模型正常工作
5. 点击**保存**

## 模型管理

### 默认模型设置

您可以设置默认的模型以用于不同类型的任务：

1. 导航至**设置 > 模型**
2. 选择要设置为默认的模型
3. 点击**设为默认**按钮

### 模型访问控制

您可以控制哪些团队成员可以访问特定模型：

1. 导航至**设置 > 模型 > 访问控制**
2. 为每个模型设置访问权限
3. 点击**保存更改**

## 模型性能比较

Dify 提供了模型性能比较工具，帮助您选择最适合您需求的模型：

1. 导航至**应用 > Prompt IDE**
2. 输入测试提示词
3. 选择要比较的模型
4. 运行测试并查看结果比较

## 自托管模型配置

### 配置兼容 OpenAI 接口的服务器模型

如果您运行的是兼容 OpenAI API 的自托管模型（如使用 vLLM, text-generation-webui 或 LM Studio）：

1. 在模型提供商中选择 **OpenAI 兼容**
2. 输入您自托管模型的 API 端点（例如：`http://your-server:8000/v1`）
3. 输入您的 API 密钥（如果有）或留空
4. 在模型 ID 中输入您的模型名称
5. 测试连接并保存

### 配置常见自托管模型

#### Ollama

```
API 基础 URL: http://your-ollama-host:11434/api
模型名称: llama3, mistral, mixtral 等
```

#### LMStudio

```
API 基础 URL: http://localhost:1234/v1
API 密钥: 不需要或按照您的设置
模型名称: 根据您加载的模型而定
```

#### LocalAI

```
API 基础 URL: http://localhost:8080/v1
模型名称: 按照您在 LocalAI 中配置的名称
```

## 故障排除

如果您在配置模型时遇到问题，请检查以下几点：

1. 确保 API 密钥正确且未过期
2. 检查 API 端点 URL 是否正确
3. 确保您的网络可以访问模型提供商的服务器
4. 检查模型提供商是否在您的地区可用
5. 对于自托管模型，确保模型服务器正在运行

如果问题仍然存在，请查看 Dify 日志或联系社区寻求帮助。 